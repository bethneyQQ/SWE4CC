#!/usr/bin/env python3
"""
ç»¼åˆæŠ¥å‘Šç”Ÿæˆå™¨ - æ•´åˆEvaluationç»“æœå’ŒPredictionsè¯¦ç»†ä¿¡æ¯

å¯å¤ç”¨çš„æŠ¥å‘Šç”Ÿæˆå·¥å…·ï¼Œæ”¯æŒï¼š
1. ä»evaluationæŠ¥å‘Šæå–ç»Ÿè®¡ä¿¡æ¯
2. ä»predictionsæ–‡ä»¶æå–æ‰€æœ‰å¯ç”¨å­—æ®µ
3. ç”Ÿæˆå¤šç»´åº¦åˆ†ææŠ¥å‘Šï¼ˆmarkdownæ ¼å¼ï¼‰
4. æ”¯æŒè‡ªå®šä¹‰åˆ†æç»´åº¦
"""

import json
import sys
import re
from pathlib import Path
from collections import defaultdict, Counter
from datetime import datetime
import argparse


class ComprehensiveReportGenerator:
    """ç»¼åˆæŠ¥å‘Šç”Ÿæˆå™¨"""

    def __init__(self, predictions_file, eval_report_file=None, log_dir=None):
        self.predictions_file = Path(predictions_file)
        self.eval_report_file = Path(eval_report_file) if eval_report_file else None
        self.log_dir = Path(log_dir) if log_dir else None

        # æ•°æ®å®¹å™¨
        self.predictions = []
        self.eval_data = {}
        self.stats = defaultdict(dict)

    def load_data(self):
        """åŠ è½½æ‰€æœ‰æ•°æ®"""
        print(f"åŠ è½½predictionsæ–‡ä»¶: {self.predictions_file}")
        self._load_predictions()

        if self.eval_report_file and self.eval_report_file.exists():
            print(f"åŠ è½½evaluationæŠ¥å‘Š: {self.eval_report_file}")
            self._load_eval_report()
        else:
            print("æœªæä¾›evaluationæŠ¥å‘Šï¼Œä»…åˆ†æpredictions")

    def _load_predictions(self):
        """åŠ è½½predictionsæ–‡ä»¶ï¼ˆJSONLæ ¼å¼ï¼‰"""
        with open(self.predictions_file, 'r') as f:
            for line_num, line in enumerate(f, 1):
                try:
                    pred = json.loads(line.strip())
                    self.predictions.append(pred)
                except json.JSONDecodeError as e:
                    print(f"è­¦å‘Š: ç¬¬{line_num}è¡ŒJSONè§£æå¤±è´¥: {e}")

        print(f"  å·²åŠ è½½ {len(self.predictions)} æ¡predictions")

    def _load_eval_report(self):
        """åŠ è½½evaluationæŠ¥å‘Šï¼ˆJSONæ ¼å¼ï¼‰"""
        with open(self.eval_report_file, 'r') as f:
            self.eval_data = json.load(f)

        print(f"  å·²åŠ è½½evaluationæŠ¥å‘Š")

    def analyze(self):
        """æ‰§è¡Œå…¨é¢åˆ†æ"""
        print("\nå¼€å§‹åˆ†æ...")

        # åŸºç¡€ç»Ÿè®¡
        self._analyze_basic_stats()

        # Predictionså­—æ®µåˆ†æ
        self._analyze_predictions_fields()

        # é”™è¯¯åˆ†æ
        self._analyze_errors()

        # é”™è¯¯æ ¹å› åˆ†æï¼ˆä»æ—¥å¿—ï¼‰
        if self.log_dir and self.log_dir.exists():
            self._analyze_error_causes()

        # å¤±è´¥ç±»å‹åˆ†ç±»
        if self.eval_data:
            self._classify_failure_types()

        # æ€§èƒ½åˆ†æï¼ˆå¦‚æœæœ‰ç›¸å…³å­—æ®µï¼‰
        self._analyze_performance()

        # æŒ‰ç»´åº¦åˆ†ç»„åˆ†æ
        self._analyze_by_dimensions()

        print("åˆ†æå®Œæˆï¼\n")

    def _analyze_basic_stats(self):
        """åŸºç¡€ç»Ÿè®¡"""
        stats = {
            'total_samples': len(self.predictions),
            'timestamp': datetime.now().isoformat(),
            'predictions_file': str(self.predictions_file.name),
        }

        # ä»evaluationæŠ¥å‘Šè·å–ç»Ÿè®¡
        if self.eval_data:
            stats.update({
                'eval_total': self.eval_data.get('total_instances', 0),
                'eval_completed': self.eval_data.get('completed_instances', 0),
                'eval_resolved': self.eval_data.get('resolved_instances', 0),
                'eval_unresolved': self.eval_data.get('unresolved_instances', 0),
                'eval_errors': self.eval_data.get('error_instances', 0),
                'eval_empty_patch': self.eval_data.get('empty_patch_instances', 0),
            })

            # è®¡ç®—é€šè¿‡ç‡
            total = stats['eval_total']
            if total > 0:
                stats['pass_rate'] = stats['eval_resolved'] / total
                stats['completion_rate'] = stats['eval_completed'] / total
                stats['error_rate'] = stats['eval_errors'] / total

        self.stats['basic'] = stats

    def _analyze_predictions_fields(self):
        """åˆ†æpredictionsä¸­çš„æ‰€æœ‰å­—æ®µ"""
        if not self.predictions:
            return

        # æ”¶é›†æ‰€æœ‰å‡ºç°çš„å­—æ®µ
        all_fields = set()
        field_presence = Counter()
        field_types = defaultdict(Counter)

        for pred in self.predictions:
            fields = set(pred.keys())
            all_fields.update(fields)

            for field in fields:
                field_presence[field] += 1
                field_types[field][type(pred[field]).__name__] += 1

        # ç»Ÿè®¡å­—æ®µè¦†ç›–ç‡
        total = len(self.predictions)
        field_stats = {}

        for field in sorted(all_fields):
            count = field_presence[field]
            types = dict(field_types[field])

            field_stats[field] = {
                'presence': count,
                'coverage': count / total,
                'types': types,
            }

        self.stats['fields'] = {
            'all_fields': sorted(all_fields),
            'field_count': len(all_fields),
            'field_details': field_stats,
        }

    def _analyze_errors(self):
        """é”™è¯¯åˆ†æ"""
        error_stats = {
            'total_with_errors': 0,
            'error_types': Counter(),
            'error_samples': [],
        }

        for pred in self.predictions:
            # æ£€æŸ¥å„ç§é”™è¯¯å­—æ®µ
            has_error = False
            error_info = {}

            if 'error' in pred and pred['error']:
                has_error = True
                error_info['error'] = pred['error']

            if 'exception' in pred and pred['exception']:
                has_error = True
                error_info['exception'] = pred['exception']

            if 'passed' in pred and pred['passed'] == False:
                has_error = True
                error_info['failed_test'] = True

            if has_error:
                error_stats['total_with_errors'] += 1

                # è®°å½•é”™è¯¯ç±»å‹
                error_type = 'unknown'
                if 'error' in error_info:
                    error_type = str(error_info['error'])[:50]
                elif 'exception' in error_info:
                    error_type = str(error_info['exception'])[:50]
                elif 'failed_test' in error_info:
                    error_type = 'test_failed'

                error_stats['error_types'][error_type] += 1

                # ä¿å­˜å‰10ä¸ªé”™è¯¯æ ·æœ¬
                if len(error_stats['error_samples']) < 10:
                    error_stats['error_samples'].append({
                        'instance_id': pred.get('instance_id', pred.get('case_id', 'unknown')),
                        'error_info': error_info,
                    })

        # è½¬æ¢Counterä¸ºdict
        error_stats['error_types'] = dict(error_stats['error_types'].most_common(20))

        self.stats['errors'] = error_stats

    def _analyze_performance(self):
        """æ€§èƒ½åˆ†æï¼ˆå»¶è¿Ÿã€æˆæœ¬ç­‰ï¼‰"""
        perf_stats = {
            'has_latency': False,
            'has_cost': False,
            'has_tokens': False,
        }

        latencies = []
        costs = []
        input_tokens = []
        output_tokens = []

        for pred in self.predictions:
            # å»¶è¿Ÿ
            if 'latency_ms' in pred:
                latencies.append(pred['latency_ms'])
            elif 'duration_s' in pred:
                latencies.append(pred['duration_s'] * 1000)

            # æˆæœ¬
            if 'cost' in pred:
                costs.append(pred['cost'])
            elif 'billing_cost' in pred:
                costs.append(pred['billing_cost'])

            # Tokens
            if 'input_tokens' in pred:
                input_tokens.append(pred['input_tokens'])
            if 'output_tokens' in pred:
                output_tokens.append(pred['output_tokens'])

        # è®¡ç®—ç»Ÿè®¡é‡
        if latencies:
            perf_stats['has_latency'] = True
            perf_stats['latency'] = self._calc_stats(latencies)

        if costs:
            perf_stats['has_cost'] = True
            perf_stats['cost'] = self._calc_stats(costs)
            perf_stats['total_cost'] = sum(costs)

        if input_tokens or output_tokens:
            perf_stats['has_tokens'] = True
            if input_tokens:
                perf_stats['input_tokens'] = self._calc_stats(input_tokens)
            if output_tokens:
                perf_stats['output_tokens'] = self._calc_stats(output_tokens)

        self.stats['performance'] = perf_stats

    def _analyze_by_dimensions(self):
        """æŒ‰ç»´åº¦åˆ†ç»„åˆ†æ"""
        dimensions = ['dataset', 'model_name', 'split', 'task', 'difficulty']

        by_dim = {}

        for dim in dimensions:
            # æ£€æŸ¥æ˜¯å¦æœ‰è¿™ä¸ªç»´åº¦
            has_dim = any(dim in pred for pred in self.predictions)
            if not has_dim:
                continue

            # æŒ‰ç»´åº¦åˆ†ç»„
            groups = defaultdict(list)
            for pred in self.predictions:
                if dim in pred:
                    key = pred[dim]
                    groups[key].append(pred)

            # è®¡ç®—æ¯ç»„çš„ç»Ÿè®¡
            dim_stats = {}
            for key, preds in groups.items():
                dim_stats[str(key)] = {
                    'count': len(preds),
                    'percentage': len(preds) / len(self.predictions),
                }

                # è®¡ç®—é€šè¿‡ç‡ï¼ˆå¦‚æœæœ‰passedå­—æ®µï¼‰
                if any('passed' in p for p in preds):
                    passed = sum(1 for p in preds if p.get('passed', False))
                    dim_stats[str(key)]['passed'] = passed
                    dim_stats[str(key)]['pass_rate'] = passed / len(preds)

            by_dim[dim] = dim_stats

        self.stats['by_dimension'] = by_dim

    def _analyze_error_causes(self):
        """ä»æ—¥å¿—æ–‡ä»¶åˆ†æé”™è¯¯æ ¹å› """
        print("  åˆ†æé”™è¯¯æ ¹å› ...")

        error_causes = {
            'malformed_patch': [],
            'docker_rate_limit': [],
            'patch_apply_failed': [],
            'test_failure': [],
            'timeout': [],
            'other': [],
        }

        error_patterns = Counter()
        error_examples = defaultdict(list)

        # è·å–æ‰€æœ‰é”™è¯¯å®ä¾‹çš„æ—¥å¿—
        error_ids = self.eval_data.get('error_ids', [])
        unresolved_ids = self.eval_data.get('unresolved_ids', [])

        for instance_id in error_ids + unresolved_ids:
            log_file = self.log_dir / instance_id / "run_instance.log"

            if not log_file.exists():
                continue

            try:
                with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                    log_content = f.read()

                # åˆ†ç±»é”™è¯¯åŸå› 
                cause = 'other'
                error_detail = ''

                # æ£€æŸ¥å„ç§é”™è¯¯æ¨¡å¼
                if 'malformed patch' in log_content:
                    cause = 'malformed_patch'
                    # æå–å…·ä½“é”™è¯¯ä¿¡æ¯
                    match = re.search(r'malformed patch at line (\d+): (.+)', log_content)
                    if match:
                        line_num, error_msg = match.groups()
                        error_detail = f"Line {line_num}: {error_msg[:50]}"
                        error_patterns[error_msg[:50]] += 1

                elif 'Too Many Requests' in log_content or 'toomanyrequests' in log_content:
                    cause = 'docker_rate_limit'
                    error_detail = "Docker Hub rate limit exceeded"

                elif 'Patch Apply Failed' in log_content:
                    cause = 'patch_apply_failed'
                    # æå–å…·ä½“é”™è¯¯
                    match = re.search(r'Patch Apply Failed:\n(.+?)(?:\n\n|Check)', log_content, re.DOTALL)
                    if match:
                        error_detail = match.group(1).strip()[:100]
                        # æå–é”™è¯¯æ¨¡å¼
                        if 'Hunk' in error_detail:
                            error_patterns['Hunk failed to apply'] += 1
                        elif 'can\'t find file' in error_detail:
                            error_patterns['File not found'] += 1
                        else:
                            error_patterns['Other patch apply error'] += 1

                elif 'resolved: False' in log_content and instance_id in unresolved_ids:
                    cause = 'test_failure'
                    # æå–æµ‹è¯•å¤±è´¥ä¿¡æ¯
                    match = re.search(r"'FAIL_TO_PASS'.*?'failure': \[(.*?)\]", log_content, re.DOTALL)
                    if match:
                        failed_tests = match.group(1)[:100]
                        error_detail = f"Tests failed: {failed_tests}"
                        error_patterns['Test failure'] += 1

                elif 'timeout' in log_content.lower():
                    cause = 'timeout'
                    error_detail = "Execution timeout"
                    error_patterns['Timeout'] += 1

                error_causes[cause].append(instance_id)

                # ä¿å­˜é”™è¯¯æ ·ä¾‹ï¼ˆæ¯ç±»æœ€å¤š5ä¸ªï¼‰
                if len(error_examples[cause]) < 5 and error_detail:
                    error_examples[cause].append({
                        'instance_id': instance_id,
                        'detail': error_detail
                    })

            except Exception as e:
                print(f"    è­¦å‘Š: æ— æ³•è¯»å–æ—¥å¿— {log_file}: {e}")
                continue

        # ä¿å­˜ç»Ÿè®¡ç»“æœ
        self.stats['error_causes'] = {
            'by_type': {k: len(v) for k, v in error_causes.items() if v},
            'instances': {k: v for k, v in error_causes.items() if v},
            'patterns': dict(error_patterns.most_common(15)),
            'examples': dict(error_examples),
        }

        print(f"    å·²åˆ†æ {sum(len(v) for v in error_causes.values())} ä¸ªé”™è¯¯å®ä¾‹")

    def _classify_failure_types(self):
        """å¯¹å¤±è´¥å®ä¾‹è¿›è¡Œè¯¦ç»†åˆ†ç±»"""
        print("  åˆ†ç±»å¤±è´¥ç±»å‹...")

        failure_types = {
            'format_errors': {
                'description': 'Patchæ ¼å¼é”™è¯¯ï¼ˆè¯­æ³•é—®é¢˜ï¼‰',
                'instances': [],
                'subcategories': Counter(),
            },
            'application_errors': {
                'description': 'Patchåº”ç”¨å¤±è´¥ï¼ˆä»£ç ä¸åŒ¹é…ï¼‰',
                'instances': [],
                'subcategories': Counter(),
            },
            'test_failures': {
                'description': 'Patchåº”ç”¨æˆåŠŸä½†æµ‹è¯•å¤±è´¥',
                'instances': [],
                'subcategories': Counter(),
            },
            'infrastructure_errors': {
                'description': 'åŸºç¡€è®¾æ–½é—®é¢˜ï¼ˆDockerç­‰ï¼‰',
                'instances': [],
                'subcategories': Counter(),
            },
        }

        # ä»error_causesè·å–åˆ†ç±»ä¿¡æ¯
        if 'error_causes' in self.stats:
            causes = self.stats['error_causes']['instances']

            # æ ¼å¼é”™è¯¯
            if 'malformed_patch' in causes:
                failure_types['format_errors']['instances'] = causes['malformed_patch']
                # ä»patternsè·å–å­åˆ†ç±»
                patterns = self.stats['error_causes']['patterns']
                for pattern, count in patterns.items():
                    if any(x in pattern.lower() for x in ['expected', 'trailing', 'unexpected', 'leading']):
                        failure_types['format_errors']['subcategories'][pattern[:40]] = count

            # åº”ç”¨é”™è¯¯
            if 'patch_apply_failed' in causes:
                failure_types['application_errors']['instances'] = causes['patch_apply_failed']
                patterns = self.stats['error_causes']['patterns']
                for pattern, count in patterns.items():
                    if 'hunk' in pattern.lower() or 'file' in pattern.lower():
                        failure_types['application_errors']['subcategories'][pattern[:40]] = count

            # æµ‹è¯•å¤±è´¥
            if 'test_failure' in causes:
                failure_types['test_failures']['instances'] = causes['test_failure']
                failure_types['test_failures']['subcategories']['Tests did not pass'] = len(causes['test_failure'])

            # åŸºç¡€è®¾æ–½é”™è¯¯
            if 'docker_rate_limit' in causes:
                failure_types['infrastructure_errors']['instances'] = causes['docker_rate_limit']
                failure_types['infrastructure_errors']['subcategories']['Docker rate limit'] = len(causes['docker_rate_limit'])

            if 'timeout' in causes:
                failure_types['infrastructure_errors']['instances'].extend(causes['timeout'])
                failure_types['infrastructure_errors']['subcategories']['Timeout'] = len(causes['timeout'])

        # è®¡ç®—ç»Ÿè®¡
        failure_summary = {}
        for ftype, data in failure_types.items():
            if data['instances']:
                failure_summary[ftype] = {
                    'count': len(data['instances']),
                    'percentage': len(data['instances']) / self.stats['basic']['total_samples'] * 100,
                    'description': data['description'],
                    'subcategories': dict(data['subcategories'].most_common(5)),
                    'example_instances': data['instances'][:5],
                }

        self.stats['failure_classification'] = failure_summary

        print(f"    å·²åˆ†ç±» {sum(v['count'] for v in failure_summary.values())} ä¸ªå¤±è´¥å®ä¾‹")

    def _calc_stats(self, values):
        """è®¡ç®—ç»Ÿè®¡é‡"""
        if not values:
            return {}

        sorted_values = sorted(values)
        n = len(sorted_values)

        return {
            'count': n,
            'min': sorted_values[0],
            'max': sorted_values[-1],
            'mean': sum(values) / n,
            'median': sorted_values[n // 2],
            'p90': sorted_values[int(n * 0.9)],
            'p95': sorted_values[int(n * 0.95)],
            'p99': sorted_values[int(n * 0.99)] if n >= 100 else sorted_values[-1],
        }

    def generate_report(self, output_file=None):
        """ç”ŸæˆMarkdownæŠ¥å‘Š"""
        if output_file is None:
            output_file = self.predictions_file.parent / f"{self.predictions_file.stem}_comprehensive_report.md"

        output_file = Path(output_file)

        print(f"ç”ŸæˆæŠ¥å‘Š: {output_file}")

        with open(output_file, 'w', encoding='utf-8') as f:
            # æ ‡é¢˜
            f.write(f"# ç»¼åˆè¯„ä¼°æŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {self.stats['basic']['timestamp']}\n\n")
            f.write(f"**Predictionsæ–‡ä»¶**: `{self.stats['basic']['predictions_file']}`\n\n")

            if self.eval_report_file:
                f.write(f"**EvaluationæŠ¥å‘Š**: `{self.eval_report_file.name}`\n\n")

            f.write("---\n\n")

            # 1. æ‰§è¡Œæ‘˜è¦
            f.write("## ğŸ“Š æ‰§è¡Œæ‘˜è¦\n\n")
            self._write_executive_summary(f)

            # 2. Evaluationç»“æœ
            if self.eval_data:
                f.write("\n## âœ… Evaluationç»“æœ\n\n")
                self._write_eval_results(f)

            # 3. Predictionså­—æ®µåˆ†æ
            f.write("\n## ğŸ” Predictionså­—æ®µåˆ†æ\n\n")
            self._write_field_analysis(f)

            # 4. é”™è¯¯åˆ†æ
            f.write("\n## âŒ é”™è¯¯åˆ†æ\n\n")
            self._write_error_analysis(f)

            # 5. é”™è¯¯æ ¹å› åˆ†æ
            if 'error_causes' in self.stats:
                f.write("\n## ğŸ”¬ é”™è¯¯æ ¹å› åˆ†æ\n\n")
                self._write_error_causes(f)

            # 6. å¤±è´¥ç±»å‹åˆ†ç±»
            if 'failure_classification' in self.stats:
                f.write("\n## ğŸ“‹ å¤±è´¥ç±»å‹åˆ†ç±»\n\n")
                self._write_failure_classification(f)

            # 7. æ€§èƒ½åˆ†æ
            if self.stats['performance']['has_latency'] or self.stats['performance']['has_cost']:
                f.write("\n## âš¡ æ€§èƒ½åˆ†æ\n\n")
                self._write_performance_analysis(f)

            # 8. ç»´åº¦åˆ†æ
            if self.stats.get('by_dimension'):
                f.write("\n## ğŸ“ˆ å¤šç»´åº¦åˆ†æ\n\n")
                self._write_dimension_analysis(f)

            # 9. å»ºè®®ä¸æ€»ç»“
            f.write("\n## ğŸ’¡ å»ºè®®ä¸æ€»ç»“\n\n")
            self._write_recommendations(f)

        print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")
        return str(output_file)

    def _write_executive_summary(self, f):
        """æ‰§è¡Œæ‘˜è¦"""
        basic = self.stats['basic']

        f.write("### æ ¸å¿ƒæŒ‡æ ‡\n\n")
        f.write(f"- **æ€»æ ·æœ¬æ•°**: {basic['total_samples']}\n")

        if 'eval_total' in basic:
            f.write(f"- **è¯„ä¼°æ ·æœ¬æ•°**: {basic['eval_total']}\n")
            f.write(f"- **æˆåŠŸè§£å†³**: {basic['eval_resolved']} ({basic.get('pass_rate', 0)*100:.1f}%)\n")
            f.write(f"- **æœªè§£å†³**: {basic['eval_unresolved']}\n")
            f.write(f"- **é”™è¯¯æ•°**: {basic['eval_errors']} ({basic.get('error_rate', 0)*100:.1f}%)\n")
            f.write(f"- **ç©ºè¡¥ä¸**: {basic['eval_empty_patch']}\n")

    def _write_eval_results(self, f):
        """Evaluationç»“æœè¯¦æƒ…"""
        f.write("### ç»Ÿè®¡è¯¦æƒ…\n\n")
        f.write("| æŒ‡æ ‡ | æ•°é‡ | ç™¾åˆ†æ¯” |\n")
        f.write("|------|------|--------|\n")

        total = self.eval_data.get('total_instances', 1)
        metrics = [
            ('æ€»å®ä¾‹æ•°', 'total_instances'),
            ('æäº¤å®ä¾‹', 'submitted_instances'),
            ('å®Œæˆè¯„ä¼°', 'completed_instances'),
            ('å·²è§£å†³', 'resolved_instances'),
            ('æœªè§£å†³', 'unresolved_instances'),
            ('é”™è¯¯', 'error_instances'),
            ('ç©ºè¡¥ä¸', 'empty_patch_instances'),
        ]

        for label, key in metrics:
            if key in self.eval_data:
                value = self.eval_data[key]
                pct = value / total * 100 if total > 0 else 0
                f.write(f"| {label} | {value} | {pct:.1f}% |\n")

        # æˆåŠŸ/å¤±è´¥çš„instance IDs
        if 'completed_ids' in self.eval_data:
            completed = self.eval_data['completed_ids']
            f.write(f"\n### æˆåŠŸå®ä¾‹ ({len(completed)}ä¸ª)\n\n")
            f.write(f"```\n{', '.join(completed[:20])}\n")
            if len(completed) > 20:
                f.write(f"... è¿˜æœ‰ {len(completed)-20} ä¸ª\n")
            f.write("```\n")

        if 'error_ids' in self.eval_data:
            errors = self.eval_data['error_ids']
            f.write(f"\n### é”™è¯¯å®ä¾‹ ({len(errors)}ä¸ª)\n\n")
            f.write(f"```\n{', '.join(errors[:20])}\n")
            if len(errors) > 20:
                f.write(f"... è¿˜æœ‰ {len(errors)-20} ä¸ª\n")
            f.write("```\n")

    def _write_field_analysis(self, f):
        """å­—æ®µåˆ†æ"""
        fields = self.stats['fields']

        f.write(f"### å­—æ®µæ¦‚è§ˆ\n\n")
        f.write(f"- **å­—æ®µæ€»æ•°**: {fields['field_count']}\n")
        f.write(f"- **æ‰€æœ‰å­—æ®µ**: `{', '.join(fields['all_fields'])}`\n\n")

        f.write("### å­—æ®µè¯¦æƒ…\n\n")
        f.write("| å­—æ®µå | è¦†ç›–ç‡ | ç±»å‹ |\n")
        f.write("|--------|--------|------|\n")

        for field, details in sorted(fields['field_details'].items(),
                                     key=lambda x: x[1]['coverage'], reverse=True):
            coverage = details['coverage'] * 100
            types = ', '.join(f"{t}({c})" for t, c in details['types'].items())
            f.write(f"| `{field}` | {coverage:.1f}% | {types} |\n")

    def _write_error_analysis(self, f):
        """é”™è¯¯åˆ†æ"""
        errors = self.stats['errors']

        f.write(f"### é”™è¯¯ç»Ÿè®¡\n\n")
        f.write(f"- **é”™è¯¯æ€»æ•°**: {errors['total_with_errors']}\n")
        f.write(f"- **é”™è¯¯ç‡**: {errors['total_with_errors']/len(self.predictions)*100:.1f}%\n\n")

        if errors['error_types']:
            f.write("### Topé”™è¯¯ç±»å‹\n\n")
            f.write("| é”™è¯¯ç±»å‹ | æ•°é‡ |\n")
            f.write("|----------|------|\n")

            for error_type, count in list(errors['error_types'].items())[:10]:
                f.write(f"| `{error_type}` | {count} |\n")

        if errors['error_samples']:
            f.write("\n### é”™è¯¯æ ·æœ¬\n\n")
            for i, sample in enumerate(errors['error_samples'][:5], 1):
                f.write(f"{i}. **{sample['instance_id']}**\n")
                f.write(f"   - {sample['error_info']}\n")

    def _write_performance_analysis(self, f):
        """æ€§èƒ½åˆ†æ"""
        perf = self.stats['performance']

        if perf.get('has_latency'):
            lat = perf['latency']
            f.write("### å»¶è¿Ÿç»Ÿè®¡\n\n")
            f.write("| æŒ‡æ ‡ | å€¼ (ms) |\n")
            f.write("|------|--------|\n")
            f.write(f"| å¹³å‡ | {lat['mean']:.2f} |\n")
            f.write(f"| ä¸­ä½æ•° | {lat['median']:.2f} |\n")
            f.write(f"| P90 | {lat['p90']:.2f} |\n")
            f.write(f"| P95 | {lat['p95']:.2f} |\n")
            f.write(f"| æœ€å° | {lat['min']:.2f} |\n")
            f.write(f"| æœ€å¤§ | {lat['max']:.2f} |\n")
            f.write("\n")

        if perf.get('has_cost'):
            cost = perf['cost']
            f.write("### æˆæœ¬ç»Ÿè®¡\n\n")
            f.write(f"- **æ€»æˆæœ¬**: ${perf['total_cost']:.4f}\n")
            f.write(f"- **å¹³å‡æˆæœ¬**: ${cost['mean']:.6f}\n")
            f.write(f"- **ä¸­ä½æˆæœ¬**: ${cost['median']:.6f}\n")
            f.write("\n")

        if perf.get('has_tokens'):
            f.write("### Tokenä½¿ç”¨\n\n")
            if 'input_tokens' in perf:
                tok = perf['input_tokens']
                f.write(f"- **è¾“å…¥Tokens**: å¹³å‡ {tok['mean']:.0f}, æ€»è®¡ {tok['mean']*tok['count']:.0f}\n")
            if 'output_tokens' in perf:
                tok = perf['output_tokens']
                f.write(f"- **è¾“å‡ºTokens**: å¹³å‡ {tok['mean']:.0f}, æ€»è®¡ {tok['mean']*tok['count']:.0f}\n")
            f.write("\n")

    def _write_dimension_analysis(self, f):
        """ç»´åº¦åˆ†æ"""
        by_dim = self.stats['by_dimension']

        for dim, groups in by_dim.items():
            f.write(f"### æŒ‰ {dim} åˆ†ç»„\n\n")
            f.write("| å€¼ | æ•°é‡ | å æ¯” | é€šè¿‡ç‡ |\n")
            f.write("|----|------|------|--------|\n")

            for key, stats in sorted(groups.items(), key=lambda x: x[1]['count'], reverse=True):
                count = stats['count']
                pct = stats['percentage'] * 100
                pass_rate = stats.get('pass_rate', 0) * 100 if 'pass_rate' in stats else 'N/A'

                if isinstance(pass_rate, float):
                    f.write(f"| {key} | {count} | {pct:.1f}% | {pass_rate:.1f}% |\n")
                else:
                    f.write(f"| {key} | {count} | {pct:.1f}% | {pass_rate} |\n")

            f.write("\n")

    def _write_error_causes(self, f):
        """å†™å…¥é”™è¯¯æ ¹å› åˆ†æ"""
        causes = self.stats['error_causes']

        f.write("### é”™è¯¯ç±»å‹ç»Ÿè®¡\n\n")
        f.write("| é”™è¯¯ç±»å‹ | æ•°é‡ | å æ¯” |\n")
        f.write("|---------|------|------|\n")

        total_errors = sum(causes['by_type'].values())
        for cause_type, count in sorted(causes['by_type'].items(), key=lambda x: x[1], reverse=True):
            pct = count / total_errors * 100 if total_errors > 0 else 0
            # ä¸­æ–‡åç§°æ˜ å°„
            type_names = {
                'malformed_patch': 'Patchæ ¼å¼é”™è¯¯',
                'docker_rate_limit': 'Dockeré™æµ',
                'patch_apply_failed': 'Patchåº”ç”¨å¤±è´¥',
                'test_failure': 'æµ‹è¯•å¤±è´¥',
                'timeout': 'æ‰§è¡Œè¶…æ—¶',
                'other': 'å…¶ä»–é”™è¯¯',
            }
            f.write(f"| {type_names.get(cause_type, cause_type)} | {count} | {pct:.1f}% |\n")

        # å¸¸è§é”™è¯¯æ¨¡å¼
        if causes['patterns']:
            f.write("\n### Topé”™è¯¯æ¨¡å¼\n\n")
            f.write("| é”™è¯¯æ¨¡å¼ | æ¬¡æ•° |\n")
            f.write("|---------|------|\n")

            for pattern, count in list(causes['patterns'].items())[:10]:
                # æ¸…ç†å’Œæˆªæ–­æ¨¡å¼å­—ç¬¦ä¸²
                clean_pattern = pattern.replace('|', '\\|').strip()
                f.write(f"| `{clean_pattern}` | {count} |\n")

        # é”™è¯¯ç¤ºä¾‹
        if causes['examples']:
            f.write("\n### å…¸å‹é”™è¯¯ç¤ºä¾‹\n\n")

            for cause_type, examples in causes['examples'].items():
                if not examples:
                    continue

                type_names = {
                    'malformed_patch': 'Patchæ ¼å¼é”™è¯¯',
                    'docker_rate_limit': 'Dockeré™æµ',
                    'patch_apply_failed': 'Patchåº”ç”¨å¤±è´¥',
                    'test_failure': 'æµ‹è¯•å¤±è´¥',
                    'timeout': 'æ‰§è¡Œè¶…æ—¶',
                    'other': 'å…¶ä»–é”™è¯¯',
                }

                f.write(f"#### {type_names.get(cause_type, cause_type)}\n\n")

                for ex in examples[:3]:
                    f.write(f"- **{ex['instance_id']}**: {ex['detail']}\n")

                f.write("\n")

    def _write_failure_classification(self, f):
        """å†™å…¥å¤±è´¥ç±»å‹åˆ†ç±»"""
        classification = self.stats['failure_classification']

        f.write("### å¤±è´¥ç±»å‹æ¦‚è§ˆ\n\n")
        f.write("| å¤±è´¥ç±»å‹ | æè¿° | æ•°é‡ | å æ¯” |\n")
        f.write("|---------|------|------|------|\n")

        for ftype, data in sorted(classification.items(), key=lambda x: x[1]['count'], reverse=True):
            f.write(f"| {ftype} | {data['description']} | {data['count']} | {data['percentage']:.1f}% |\n")

        # è¯¦ç»†å­åˆ†ç±»
        f.write("\n### å¤±è´¥å­ç±»å‹è¯¦æƒ…\n\n")

        for ftype, data in sorted(classification.items(), key=lambda x: x[1]['count'], reverse=True):
            if not data['subcategories']:
                continue

            # ç±»å‹åç§°æ˜ å°„
            type_display = {
                'format_errors': 'ğŸ“ æ ¼å¼é”™è¯¯',
                'application_errors': 'âš™ï¸ åº”ç”¨é”™è¯¯',
                'test_failures': 'âŒ æµ‹è¯•å¤±è´¥',
                'infrastructure_errors': 'ğŸ—ï¸ åŸºç¡€è®¾æ–½é”™è¯¯',
            }

            f.write(f"#### {type_display.get(ftype, ftype)}\n\n")
            f.write(f"**{data['description']}** ({data['count']} ä¸ªå®ä¾‹)\n\n")

            if data['subcategories']:
                f.write("| å­ç±»å‹ | æ•°é‡ |\n")
                f.write("|--------|------|\n")

                for subtype, count in data['subcategories'].items():
                    clean_subtype = subtype.replace('|', '\\|')
                    f.write(f"| {clean_subtype} | {count} |\n")

            # ç¤ºä¾‹å®ä¾‹
            if data['example_instances']:
                f.write(f"\n**ç¤ºä¾‹å®ä¾‹**: `{', '.join(data['example_instances'][:5])}`\n")

            f.write("\n")

    def _write_recommendations(self, f):
        """å»ºè®®ä¸æ€»ç»“"""
        basic = self.stats['basic']
        errors = self.stats['errors']

        f.write("### ä¸»è¦å‘ç°\n\n")

        # æˆåŠŸç‡åˆ†æ
        if 'pass_rate' in basic:
            pass_rate = basic['pass_rate'] * 100
            if pass_rate < 50:
                f.write(f"âš ï¸ **é€šè¿‡ç‡è¾ƒä½** ({pass_rate:.1f}%)ï¼Œéœ€è¦æ”¹è¿›æ¨¡å‹æˆ–ä»»åŠ¡è®¾ç½®\n\n")
            elif pass_rate < 80:
                f.write(f"âœ… **é€šè¿‡ç‡ä¸­ç­‰** ({pass_rate:.1f}%)ï¼Œæœ‰æå‡ç©ºé—´\n\n")
            else:
                f.write(f"ğŸ‰ **é€šè¿‡ç‡ä¼˜ç§€** ({pass_rate:.1f}%)ï¼\n\n")

        # é”™è¯¯ç‡åˆ†æ
        error_rate = errors['total_with_errors'] / len(self.predictions) * 100
        if error_rate > 30:
            f.write(f"âš ï¸ **é”™è¯¯ç‡è¾ƒé«˜** ({error_rate:.1f}%)ï¼Œå»ºè®®ä¼˜å…ˆä¿®å¤topé”™è¯¯ç±»å‹\n\n")

        f.write("### æ”¹è¿›å»ºè®®\n\n")

        recommendation_num = 1

        # åŸºäºå¤±è´¥åˆ†ç±»çš„å»ºè®®
        if 'failure_classification' in self.stats:
            classification = self.stats['failure_classification']

            # æŒ‰å½±å“æ’åº
            sorted_failures = sorted(classification.items(), key=lambda x: x[1]['count'], reverse=True)

            for ftype, data in sorted_failures[:3]:  # åªæ˜¾ç¤ºå‰3ç±»
                f.write(f"{recommendation_num}. **{data['description']}** ({data['count']}ä¸ª, {data['percentage']:.1f}%)\n")

                if ftype == 'format_errors':
                    f.write("   - å»ºè®®æ”¹è¿›Patchç”Ÿæˆå™¨ï¼Œç¡®ä¿ç¬¦åˆunified diffæ ¼å¼è§„èŒƒ\n")
                    f.write("   - ä½¿ç”¨patchæ ¼å¼éªŒè¯å·¥å…·æ£€æŸ¥ç”Ÿæˆçš„patches\n")
                elif ftype == 'application_errors':
                    f.write("   - æ”¹è¿›ä»£ç ä¸Šä¸‹æ–‡ç†è§£ï¼Œç¡®ä¿patchèƒ½åŒ¹é…å®é™…ä»£ç \n")
                    f.write("   - å¢åŠ ä»£ç å˜æ›´å‰çš„éªŒè¯æ­¥éª¤\n")
                elif ftype == 'test_failures':
                    f.write("   - åˆ†ææµ‹è¯•å¤±è´¥åŸå› ï¼Œå¯èƒ½æ˜¯patché€»è¾‘ä¸æ­£ç¡®\n")
                    f.write("   - è€ƒè™‘åœ¨ç”Ÿæˆpatchå‰è¿è¡Œç›¸å…³æµ‹è¯•\n")
                elif ftype == 'infrastructure_errors':
                    f.write("   - é…ç½®Dockerè®¤è¯ä»¥é¿å…rate limit\n")
                    f.write("   - å¢åŠ é‡è¯•æœºåˆ¶å’Œè¶…æ—¶å¤„ç†\n")

                f.write("\n")
                recommendation_num += 1

        # åŸºäºé”™è¯¯ç±»å‹çš„å»ºè®®ï¼ˆå¦‚æœæ²¡æœ‰åˆ†ç±»æ•°æ®ï¼‰
        elif errors['error_types']:
            top_error = list(errors['error_types'].keys())[0]
            f.write(f"{recommendation_num}. **ä¼˜å…ˆä¿®å¤**: `{top_error}` (å é”™è¯¯çš„ {list(errors['error_types'].values())[0]/errors['total_with_errors']*100:.1f}%)\n\n")
            recommendation_num += 1

        # åŸºäºå­—æ®µçš„å»ºè®®
        fields = self.stats['fields']
        missing_important = []
        for field in ['model_name', 'model_args', 'cost', 'latency_ms']:
            if field not in fields['all_fields']:
                missing_important.append(field)

        if missing_important:
            f.write(f"{recommendation_num}. **å»ºè®®æ·»åŠ å­—æ®µ**: `{', '.join(missing_important)}` ä»¥ä¾¿æ›´å…¨é¢çš„åˆ†æ\n\n")
            recommendation_num += 1

        # åŸºäºæˆæœ¬å’Œæ€§èƒ½çš„å»ºè®®
        if 'performance' in self.stats and self.stats['performance'].get('has_cost'):
            total_cost = self.stats['performance']['total_cost']
            avg_cost = self.stats['performance']['cost']['mean']
            if avg_cost > 0.15:  # å¹³å‡æˆæœ¬è¾ƒé«˜
                f.write(f"{recommendation_num}. **æˆæœ¬ä¼˜åŒ–**: å½“å‰å¹³å‡æˆæœ¬ ${avg_cost:.4f}/å®ä¾‹ï¼Œè€ƒè™‘ä½¿ç”¨æ›´å°çš„æ¨¡å‹æˆ–ä¼˜åŒ–æç¤ºè¯\n\n")

        f.write("\n---\n\n")
        f.write("*æŠ¥å‘Šç”± `generate_comprehensive_report.py` è‡ªåŠ¨ç”Ÿæˆ*\n")


def main():
    parser = argparse.ArgumentParser(
        description='ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š - æ•´åˆEvaluationç»“æœå’ŒPredictionsè¯¦ç»†ä¿¡æ¯'
    )
    parser.add_argument(
        '--predictions',
        required=True,
        help='Predictionsæ–‡ä»¶è·¯å¾„ (JSONLæ ¼å¼)'
    )
    parser.add_argument(
        '--eval-report',
        help='EvaluationæŠ¥å‘Šæ–‡ä»¶è·¯å¾„ (JSONæ ¼å¼)'
    )
    parser.add_argument(
        '--log-dir',
        help='è¯„ä¼°æ—¥å¿—ç›®å½• (ç”¨äºé”™è¯¯æ ¹å› åˆ†æ)'
    )
    parser.add_argument(
        '--output',
        help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„ (é»˜è®¤: predictionsæ–‡ä»¶åŒç›®å½•ä¸‹çš„ *_comprehensive_report.md)'
    )

    args = parser.parse_args()

    # éªŒè¯æ–‡ä»¶å­˜åœ¨
    if not Path(args.predictions).exists():
        print(f"é”™è¯¯: Predictionsæ–‡ä»¶ä¸å­˜åœ¨: {args.predictions}")
        sys.exit(1)

    if args.eval_report and not Path(args.eval_report).exists():
        print(f"é”™è¯¯: EvaluationæŠ¥å‘Šä¸å­˜åœ¨: {args.eval_report}")
        sys.exit(1)

    if args.log_dir and not Path(args.log_dir).exists():
        print(f"è­¦å‘Š: æ—¥å¿—ç›®å½•ä¸å­˜åœ¨: {args.log_dir}ï¼Œå°†è·³è¿‡é”™è¯¯æ ¹å› åˆ†æ")
        args.log_dir = None

    # ç”ŸæˆæŠ¥å‘Š
    generator = ComprehensiveReportGenerator(
        predictions_file=args.predictions,
        eval_report_file=args.eval_report,
        log_dir=args.log_dir
    )

    generator.load_data()
    generator.analyze()
    report_file = generator.generate_report(output_file=args.output)

    print(f"\nâœ… ç»¼åˆæŠ¥å‘Šç”Ÿæˆå®Œæˆ: {report_file}")


if __name__ == '__main__':
    main()
