#!/usr/bin/env python3
"""
æµ‹è¯•æ–°Promptåœ¨å¤±è´¥å®ä¾‹ä¸Šçš„æ•ˆæœ

ç”¨æ³•:
    python3 scripts/test_new_prompt.py --sample-size 10 --error-type all
"""

import json
import argparse
import random
from pathlib import Path
from collections import defaultdict


def load_error_instances(eval_report_path):
    """åŠ è½½é”™è¯¯å®ä¾‹å¹¶æŒ‰ç±»å‹åˆ†ç±»"""
    with open(eval_report_path) as f:
        eval_data = json.load(f)

    # ä»æ—¥å¿—ç›®å½•åˆ†æé”™è¯¯ç±»å‹
    log_dir = Path("logs/run_evaluation/full_eval/claude-3.5-sonnet")

    error_types = {
        'format_errors': [],
        'application_errors': [],
        'test_failures': [],
        'docker_errors': [],
    }

    for instance_id in eval_data.get('error_ids', []):
        log_file = log_dir / instance_id / "run_instance.log"
        if not log_file.exists():
            continue

        log_content = log_file.read_text(errors='ignore')

        if 'malformed patch' in log_content:
            error_types['format_errors'].append(instance_id)
        elif 'Patch Apply Failed' in log_content:
            error_types['application_errors'].append(instance_id)
        elif 'Too Many Requests' in log_content or 'toomanyrequests' in log_content:
            error_types['docker_errors'].append(instance_id)

    # æµ‹è¯•å¤±è´¥
    for instance_id in eval_data.get('unresolved_ids', []):
        log_file = log_dir / instance_id / "run_instance.log"
        if log_file.exists() and 'resolved: False' in log_file.read_text(errors='ignore'):
            error_types['test_failures'].append(instance_id)

    return error_types


def select_test_instances(error_types, sample_size, error_type='all'):
    """é€‰æ‹©æµ‹è¯•å®ä¾‹"""

    if error_type == 'all':
        # ä»æ¯ä¸ªç±»å‹æŒ‰æ¯”ä¾‹é‡‡æ ·
        samples = []

        # æ ¼å¼é”™è¯¯ (æœ€å¤šï¼Œé‡‡æ ·40%)
        n_format = min(len(error_types['format_errors']), int(sample_size * 0.4))
        samples.extend(random.sample(error_types['format_errors'], n_format))

        # åº”ç”¨å¤±è´¥ (é‡‡æ ·20%)
        n_app = min(len(error_types['application_errors']), int(sample_size * 0.2))
        samples.extend(random.sample(error_types['application_errors'], n_app))

        # æµ‹è¯•å¤±è´¥ (é‡‡æ ·40%)
        n_test = min(len(error_types['test_failures']), int(sample_size * 0.4))
        samples.extend(random.sample(error_types['test_failures'], n_test))

        # å¡«å……åˆ°ç›®æ ‡æ•°é‡
        remaining = sample_size - len(samples)
        if remaining > 0:
            all_errors = []
            for instances in error_types.values():
                all_errors.extend([i for i in instances if i not in samples])
            if all_errors:
                samples.extend(random.sample(all_errors, min(remaining, len(all_errors))))

        return samples[:sample_size]
    else:
        # ä»ç‰¹å®šç±»å‹é‡‡æ ·
        instances = error_types.get(error_type, [])
        return random.sample(instances, min(sample_size, len(instances)))


def create_test_dataset(instances, output_path):
    """åˆ›å»ºæµ‹è¯•æ•°æ®é›†æ–‡ä»¶"""
    dataset_path = Path("princeton-nlp/SWE-bench_Lite")

    # æŸ¥æ‰¾åŸå§‹æ•°æ®é›†
    possible_paths = [
        dataset_path / "test.json",
        Path("data/SWE-bench_Lite/test.json"),
    ]

    original_data = None
    for path in possible_paths:
        if path.exists():
            with open(path) as f:
                original_data = json.load(f)
            break

    if original_data is None:
        print("è­¦å‘Š: æ‰¾ä¸åˆ°åŸå§‹æ•°æ®é›†ï¼Œå°†åªåˆ›å»ºinstance_idåˆ—è¡¨")
        output_path.write_text('\n'.join(instances))
        return

    # æå–æµ‹è¯•å®ä¾‹
    test_data = [item for item in original_data if item['instance_id'] in instances]

    with open(output_path, 'w') as f:
        json.dump(test_data, f, indent=2)

    print(f"âœ… æµ‹è¯•æ•°æ®é›†å·²åˆ›å»º: {output_path} ({len(test_data)} ä¸ªå®ä¾‹)")


def generate_test_report(error_types, selected_instances):
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""

    # ç»Ÿè®¡é€‰ä¸­çš„å®ä¾‹ç±»å‹åˆ†å¸ƒ
    type_counts = defaultdict(int)
    for etype, instances in error_types.items():
        for instance in selected_instances:
            if instance in instances:
                type_counts[etype] += 1

    report = f"""
# æ–°Promptæµ‹è¯•æ–¹æ¡ˆ

## æµ‹è¯•æ ·æœ¬åˆ†å¸ƒ

æ€»æµ‹è¯•å®ä¾‹: {len(selected_instances)}

"""

    for etype, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True):
        pct = count / len(selected_instances) * 100
        report += f"- {etype}: {count} ({pct:.1f}%)\n"

    report += f"""

## æµ‹è¯•å®ä¾‹åˆ—è¡¨

```
{chr(10).join(selected_instances)}
```

## æ‰§è¡Œæ­¥éª¤

### 1. ç”¨æ–°prompté‡æ–°ç”Ÿæˆpredictions

```bash
python3 swebench/inference/run_claude_code.py \\
  --dataset_name princeton-nlp/SWE-bench_Lite \\
  --split test \\
  --instance_ids {','.join(selected_instances[:5])}... \\
  --model_name_or_path claude-3.5-sonnet \\
  --output_dir results/test_new_prompt \\
  --max_workers 4
```

### 2. è¯„ä¼°æ–°ç»“æœ

```bash
python3 swebench/harness/run_evaluation.py \\
  --predictions_path results/test_new_prompt/claude-3.5-sonnet__SWE-bench_Lite__test.jsonl \\
  --run_id test_new_prompt \\
  --max_workers 4
```

### 3. ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š

```bash
python3 scripts/generate_comprehensive_report.py \\
  --predictions results/test_new_prompt/claude-3.5-sonnet__SWE-bench_Lite__test.jsonl \\
  --eval-report reports/test_new_prompt.json \\
  --log-dir logs/run_evaluation/test_new_prompt \\
  --output reports/test_new_prompt_comprehensive.md
```

## é¢„æœŸç»“æœ

### å½“å‰è¡¨ç° (æ—§prompt)

- æ ¼å¼é”™è¯¯ç‡: {type_counts['format_errors']/len(selected_instances)*100:.1f}%
- åº”ç”¨å¤±è´¥ç‡: {type_counts['application_errors']/len(selected_instances)*100:.1f}%
- æµ‹è¯•å¤±è´¥ç‡: {type_counts['test_failures']/len(selected_instances)*100:.1f}%

### ç›®æ ‡è¡¨ç° (æ–°prompt)

- æ ¼å¼é”™è¯¯ç‡: < 10% (é™ä½ {type_counts['format_errors']/len(selected_instances)*100 - 10:.1f} ä¸ªç™¾åˆ†ç‚¹)
- åº”ç”¨å¤±è´¥ç‡: < 5% (é™ä½ {type_counts['application_errors']/len(selected_instances)*100 - 5:.1f} ä¸ªç™¾åˆ†ç‚¹)
- æ€»é€šè¿‡ç‡: > 50% (æå‡çº¦ 30 ä¸ªç™¾åˆ†ç‚¹)

## æˆæœ¬ä¼°ç®—

- é¢„è®¡æˆæœ¬: {len(selected_instances)} Ã— $0.15 = ${len(selected_instances) * 0.15:.2f}
- é¢„è®¡æ—¶é—´: {len(selected_instances)} Ã— 30ç§’ = {len(selected_instances) * 30 / 60:.1f} åˆ†é’Ÿ
"""

    return report


def main():
    parser = argparse.ArgumentParser(description='æµ‹è¯•æ–°Promptæ•ˆæœ')
    parser.add_argument(
        '--sample-size',
        type=int,
        default=20,
        help='æµ‹è¯•æ ·æœ¬æ•°é‡ (é»˜è®¤: 20)'
    )
    parser.add_argument(
        '--error-type',
        choices=['all', 'format_errors', 'application_errors', 'test_failures', 'docker_errors'],
        default='all',
        help='é”™è¯¯ç±»å‹ç­›é€‰ (é»˜è®¤: all)'
    )
    parser.add_argument(
        '--eval-report',
        default='reports/claude-3.5-sonnet.full_eval.json',
        help='è¯„ä¼°æŠ¥å‘Šè·¯å¾„'
    )
    parser.add_argument(
        '--output-dir',
        default='test_data',
        help='è¾“å‡ºç›®å½•'
    )
    parser.add_argument(
        '--seed',
        type=int,
        default=42,
        help='éšæœºç§å­'
    )

    args = parser.parse_args()

    # è®¾ç½®éšæœºç§å­
    random.seed(args.seed)

    print(f"ğŸ“Š åŠ è½½é”™è¯¯å®ä¾‹åˆ†æ...")
    error_types = load_error_instances(args.eval_report)

    # æ‰“å°ç»Ÿè®¡
    print(f"\né”™è¯¯ç±»å‹åˆ†å¸ƒ:")
    for etype, instances in error_types.items():
        print(f"  - {etype}: {len(instances)}")

    print(f"\nğŸ¯ é€‰æ‹©æµ‹è¯•æ ·æœ¬ (ç±»å‹: {args.error_type}, æ•°é‡: {args.sample_size})...")
    selected = select_test_instances(error_types, args.sample_size, args.error_type)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)

    # ä¿å­˜å®ä¾‹åˆ—è¡¨
    instances_file = output_dir / "test_instances.txt"
    instances_file.write_text('\n'.join(selected))
    print(f"âœ… å®ä¾‹åˆ—è¡¨å·²ä¿å­˜: {instances_file}")

    # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
    dataset_file = output_dir / "test_dataset.json"
    create_test_dataset(selected, dataset_file)

    # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    report = generate_test_report(error_types, selected)
    report_file = output_dir / "test_plan.md"
    report_file.write_text(report)
    print(f"âœ… æµ‹è¯•è®¡åˆ’å·²ç”Ÿæˆ: {report_file}")

    print(f"\nğŸ“‹ æµ‹è¯•æ¦‚è¦:")
    print(f"   - æµ‹è¯•å®ä¾‹: {len(selected)}")
    print(f"   - é¢„è®¡æˆæœ¬: ${len(selected) * 0.15:.2f}")
    print(f"   - é¢„è®¡æ—¶é—´: {len(selected) * 30 / 60:.1f} åˆ†é’Ÿ")
    print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥: æŸ¥çœ‹ {report_file} äº†è§£è¯¦ç»†æ‰§è¡Œæ­¥éª¤")


if __name__ == '__main__':
    main()
