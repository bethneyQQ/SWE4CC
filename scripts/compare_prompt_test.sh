#!/bin/bash
# å¯¹æ¯”æ–°æ—§Promptåœ¨3ä¸ªæ ¼å¼é”™è¯¯å®ä¾‹ä¸Šçš„æ•ˆæœ
#
# æµ‹è¯•å®ä¾‹:
# 1. astropy__astropy-14995 - æ ¼å¼é”™è¯¯
# 2. django__django-11001 - æ ¼å¼é”™è¯¯
# 3. django__django-11019 - æ ¼å¼é”™è¯¯

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

# é¢œè‰²è¾“å‡º
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m' # No Color

echo -e "${BLUE}========================================${NC}"
echo -e "${BLUE}   Promptå¯¹æ¯”æµ‹è¯• - 3ä¸ªæ ¼å¼é”™è¯¯å®ä¾‹${NC}"
echo -e "${BLUE}========================================${NC}\n"

# æµ‹è¯•å®ä¾‹ID
TEST_INSTANCES="astropy__astropy-14995,django__django-11001,django__django-11019"

echo -e "${YELLOW}æµ‹è¯•å®ä¾‹:${NC}"
echo "  1. astropy__astropy-14995"
echo "  2. django__django-11001"
echo "  3. django__django-11019"
echo ""

# åˆ›å»ºæµ‹è¯•ç›®å½•
TEST_DIR="test_prompt_comparison"
mkdir -p "$TEST_DIR"

echo -e "${BLUE}æ­¥éª¤ 1: å‡†å¤‡æ—§Promptçš„ç»“æœ${NC}"
echo "ä»ç°æœ‰predictionsä¸­æå–è¿™3ä¸ªå®ä¾‹çš„ç»“æœ..."

python3 << 'PYTHON_SCRIPT'
import json
from pathlib import Path

# è¯»å–åŸå§‹predictions
with open('results/claude-3.5-sonnet__SWE-bench_Lite_oracle__test.jsonl') as f:
    all_preds = [json.loads(line) for line in f]

# æå–æµ‹è¯•å®ä¾‹
test_ids = ['astropy__astropy-14995', 'django__django-11001', 'django__django-11019']
old_preds = [p for p in all_preds if p['instance_id'] in test_ids]

# ä¿å­˜
output_dir = Path('test_prompt_comparison')
output_dir.mkdir(exist_ok=True)

with open(output_dir / 'old_prompt_predictions.jsonl', 'w') as f:
    for pred in old_preds:
        f.write(json.dumps(pred) + '\n')

print(f"âœ… å·²ä¿å­˜ {len(old_preds)} ä¸ªæ—§Promptçš„predictions")

# åˆ›å»ºå¯¹æ¯”æŠ¥å‘Šçš„è¡¨å¤´
with open(output_dir / 'comparison_summary.md', 'w') as f:
    f.write("# Promptå¯¹æ¯”æµ‹è¯•ç»“æœ\n\n")
    f.write("## æµ‹è¯•å®ä¾‹\n\n")
    for i, inst_id in enumerate(test_ids, 1):
        f.write(f"{i}. `{inst_id}`\n")
    f.write("\n---\n\n")
PYTHON_SCRIPT

echo -e "${GREEN}âœ“ æ—§Promptç»“æœå·²æå–${NC}\n"

echo -e "${BLUE}æ­¥éª¤ 2: ç”¨æ–°Promptç”Ÿæˆpredictions${NC}"
echo "è°ƒç”¨Claude Codeä¸º3ä¸ªå®ä¾‹ç”Ÿæˆpatches (ä½¿ç”¨æ–°Prompt)..."
echo ""

# ä½¿ç”¨ä¸“é—¨çš„è„šæœ¬ï¼Œåªå¤„ç†è¿™3ä¸ªå®ä¾‹
python3 scripts/run_single_instances.py \
  --instance_ids "$TEST_INSTANCES" \
  --model claude-3.5-sonnet \
  --output_dir "$TEST_DIR/new_prompt_results"

echo -e "${GREEN}âœ“ æ–°Prompt predictionså·²ç”Ÿæˆ${NC}\n"

echo -e "${BLUE}æ­¥éª¤ 3: è¯„ä¼°æ–°Promptçš„ç»“æœ${NC}"

python3 swebench/harness/run_evaluation.py \
  --predictions_path "$TEST_DIR/new_prompt_results"/*.jsonl \
  --run_id new_prompt_test \
  --max_workers 4

echo -e "${GREEN}âœ“ æ–°Promptè¯„ä¼°å®Œæˆ${NC}\n"

echo -e "${BLUE}æ­¥éª¤ 4: ç”Ÿæˆå¯¹æ¯”åˆ†æ${NC}"

python3 << 'PYTHON_SCRIPT'
import json
from pathlib import Path
from datetime import datetime

test_dir = Path('test_prompt_comparison')

# åŠ è½½æ—§ç»“æœ
with open('reports/claude-3.5-sonnet.full_eval.json') as f:
    old_eval = json.load(f)

# åŠ è½½æ–°ç»“æœ
new_eval_path = Path('reports/new_prompt_test.json')
if new_eval_path.exists():
    with open(new_eval_path) as f:
        new_eval = json.load(f)
else:
    print("è­¦å‘Š: æ–°è¯„ä¼°ç»“æœæœªæ‰¾åˆ°")
    new_eval = {}

test_ids = ['astropy__astropy-14995', 'django__django-11001', 'django__django-11019']

# ç”Ÿæˆè¯¦ç»†å¯¹æ¯”æŠ¥å‘Š
report = f"""# Promptå¯¹æ¯”æµ‹è¯•ç»“æœ

**æµ‹è¯•æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## æµ‹è¯•æ¦‚å†µ

- **æµ‹è¯•å®ä¾‹æ•°**: 3
- **å®ä¾‹ç±»å‹**: æ ¼å¼é”™è¯¯ (malformed patch)
- **å¯¹æ¯”ç»´åº¦**: æ—§Prompt vs æ–°Prompt

---

## æµ‹è¯•å®ä¾‹è¯¦æƒ…

"""

# å¯¹æ¯ä¸ªå®ä¾‹è¿›è¡Œå¯¹æ¯”
for idx, inst_id in enumerate(test_ids, 1):
    report += f"### {idx}. {inst_id}\n\n"

    # æ—§ç»“æœçŠ¶æ€
    old_status = "âŒ ERROR"
    if inst_id in old_eval.get('resolved_ids', []):
        old_status = "âœ… RESOLVED"
    elif inst_id in old_eval.get('unresolved_ids', []):
        old_status = "âš ï¸ UNRESOLVED"

    # æ–°ç»“æœçŠ¶æ€
    new_status = "ğŸ”„ TESTING"
    if new_eval:
        if inst_id in new_eval.get('resolved_ids', []):
            new_status = "âœ… RESOLVED"
        elif inst_id in new_eval.get('unresolved_ids', []):
            new_status = "âš ï¸ UNRESOLVED"
        elif inst_id in new_eval.get('error_ids', []):
            new_status = "âŒ ERROR"

    report += f"| Promptç‰ˆæœ¬ | çŠ¶æ€ |\n"
    report += f"|-----------|------|\n"
    report += f"| æ—§Prompt | {old_status} |\n"
    report += f"| æ–°Prompt | {new_status} |\n\n"

    # æ£€æŸ¥æ—§çš„é”™è¯¯æ—¥å¿—
    old_log = Path(f'logs/run_evaluation/full_eval/claude-3.5-sonnet/{inst_id}/run_instance.log')
    if old_log.exists():
        log_content = old_log.read_text(errors='ignore')
        if 'malformed patch' in log_content:
            import re
            match = re.search(r'malformed patch at line (\d+): (.+)', log_content)
            if match:
                report += f"**æ—§Prompté”™è¯¯**: Line {match.group(1)}: `{match.group(2)[:60]}`\n\n"

    # æ£€æŸ¥æ–°çš„é”™è¯¯æ—¥å¿—
    new_log = Path(f'logs/run_evaluation/new_prompt_test/{inst_id}/run_instance.log')
    if new_log.exists():
        log_content = new_log.read_text(errors='ignore')
        if 'malformed patch' in log_content:
            import re
            match = re.search(r'malformed patch at line (\d+): (.+)', log_content)
            if match:
                report += f"**æ–°Prompté”™è¯¯**: Line {match.group(1)}: `{match.group(2)[:60]}`\n\n"
        elif 'resolved: True' in log_content:
            report += f"**æ–°Prompt**: âœ… Patchæ ¼å¼æ­£ç¡®ï¼ŒæˆåŠŸè§£å†³é—®é¢˜ï¼\n\n"
        elif 'resolved: False' in log_content:
            report += f"**æ–°Prompt**: âš ï¸ Patchæ ¼å¼æ­£ç¡®ä½†æµ‹è¯•æœªé€šè¿‡\n\n"

    report += "---\n\n"

# æ±‡æ€»ç»Ÿè®¡
report += """## æ±‡æ€»å¯¹æ¯”

| æŒ‡æ ‡ | æ—§Prompt | æ–°Prompt | å˜åŒ– |
|------|---------|----------|------|
"""

old_errors = sum(1 for id in test_ids if id in old_eval.get('error_ids', []))
old_resolved = sum(1 for id in test_ids if id in old_eval.get('resolved_ids', []))
old_unresolved = sum(1 for id in test_ids if id in old_eval.get('unresolved_ids', []))

new_errors = 0
new_resolved = 0
new_unresolved = 0

if new_eval:
    new_errors = sum(1 for id in test_ids if id in new_eval.get('error_ids', []))
    new_resolved = sum(1 for id in test_ids if id in new_eval.get('resolved_ids', []))
    new_unresolved = sum(1 for id in test_ids if id in new_eval.get('unresolved_ids', []))

report += f"| æ ¼å¼é”™è¯¯ | {old_errors}/3 | {new_errors}/3 | {'-' + str(old_errors - new_errors) if old_errors > new_errors else '+' + str(new_errors - old_errors)} |\n"
report += f"| æˆåŠŸè§£å†³ | {old_resolved}/3 | {new_resolved}/3 | {'+' + str(new_resolved - old_resolved) if new_resolved > old_resolved else str(new_resolved - old_resolved)} |\n"
report += f"| æµ‹è¯•å¤±è´¥ | {old_unresolved}/3 | {new_unresolved}/3 | {'+' + str(new_unresolved - old_unresolved) if new_unresolved > old_unresolved else str(new_unresolved - old_unresolved)} |\n"

report += "\n---\n\n"

# ç»“è®º
if new_eval:
    improvement = (new_resolved - old_resolved) + ((old_errors - new_errors) * 0.5)
    if improvement > 0:
        report += f"## âœ… ç»“è®º\n\næ–°Promptè¡¨ç°**æ›´å¥½**ï¼Œæ”¹å–„äº† {improvement:.1f} ä¸ªå®ä¾‹\n\n"
    elif improvement < 0:
        report += f"## âš ï¸ ç»“è®º\n\næ–°Promptè¡¨ç°**æ›´å·®**ï¼Œé€€åŒ–äº† {-improvement:.1f} ä¸ªå®ä¾‹\n\n"
    else:
        report += f"## ğŸ“Š ç»“è®º\n\næ–°æ—§Promptè¡¨ç°**ç›¸å½“**\n\n"

    if new_errors < old_errors:
        report += f"æ ¼å¼é”™è¯¯æ˜¾è‘—å‡å°‘: {old_errors} â†’ {new_errors}\n\n"

report += """## ä¸‹ä¸€æ­¥å»ºè®®

"""

if new_eval and new_errors < old_errors:
    report += "- âœ… æ–°Promptåœ¨æ ¼å¼æ­£ç¡®æ€§ä¸Šæœ‰æ”¹å–„ï¼Œå»ºè®®æ‰©å¤§æµ‹è¯•èŒƒå›´\n"
    report += "- å»ºè®®æµ‹è¯•æ›´å¤šå®ä¾‹ (20-50ä¸ª) æ¥ç¡®è®¤æ•ˆæœ\n"
else:
    report += "- éœ€è¦è¿›ä¸€æ­¥åˆ†ææ–°Promptä¸ºä½•æ²¡æœ‰æ”¹å–„æ ¼å¼é—®é¢˜\n"
    report += "- æ£€æŸ¥patchæå–é€»è¾‘æ˜¯å¦æ­£ç¡®\n"

# ä¿å­˜æŠ¥å‘Š
with open(test_dir / 'comparison_report.md', 'w') as f:
    f.write(report)

print("âœ… å¯¹æ¯”æŠ¥å‘Šå·²ç”Ÿæˆ")
print("")
print("=" * 50)
print(report)
print("=" * 50)

PYTHON_SCRIPT

echo ""
echo -e "${GREEN}========================================${NC}"
echo -e "${GREEN}   æµ‹è¯•å®Œæˆï¼${NC}"
echo -e "${GREEN}========================================${NC}"
echo ""
echo "ğŸ“Š æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š:"
echo "   cat test_prompt_comparison/comparison_report.md"
echo ""
echo "ğŸ“ ç›¸å…³æ–‡ä»¶:"
echo "   - æ—§Promptç»“æœ: test_prompt_comparison/old_prompt_predictions.jsonl"
echo "   - æ–°Promptç»“æœ: test_prompt_comparison/new_prompt_results/*.jsonl"
echo "   - è¯„ä¼°æ—¥å¿—: logs/run_evaluation/new_prompt_test/"
echo ""
