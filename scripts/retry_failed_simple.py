#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆé‡è¯•è„šæœ¬ - ç›´æ¥è°ƒç”¨run_api.pyå¤„ç†ç‰¹å®šå®ä¾‹

åˆ›å»ºä¸´æ—¶æ•°æ®é›†æ–‡ä»¶ï¼ŒåªåŒ…å«å¤±è´¥çš„å®ä¾‹
"""

import argparse
import json
import sys
import subprocess
from pathlib import Path
from datasets import Dataset, DatasetDict

def create_temp_dataset(failed_instances, original_dataset_name, temp_path):
    """åˆ›å»ºåªåŒ…å«å¤±è´¥å®ä¾‹çš„ä¸´æ—¶æ•°æ®é›†"""
    from datasets import load_dataset

    print(f"åŠ è½½åŸå§‹æ•°æ®é›†: {original_dataset_name}")
    dataset = load_dataset(original_dataset_name, split="test")

    # è¿‡æ»¤å‡ºå¤±è´¥çš„å®ä¾‹
    filtered = dataset.filter(lambda x: x['instance_id'] in failed_instances)

    print(f"åŸå§‹æ•°æ®é›†: {len(dataset)} å®ä¾‹")
    print(f"å¤±è´¥å®ä¾‹: {len(failed_instances)} ä¸ª")
    print(f"åŒ¹é…åˆ°: {len(filtered)} å®ä¾‹")

    # ä¿å­˜ä¸ºä¸´æ—¶æ•°æ®é›†
    temp_path = Path(temp_path)
    temp_path.mkdir(parents=True, exist_ok=True)

    filtered.save_to_disk(str(temp_path))
    print(f"ä¸´æ—¶æ•°æ®é›†å·²ä¿å­˜åˆ°: {temp_path}")

    return len(filtered)

def main():
    parser = argparse.ArgumentParser(description='é‡è¯•å¤±è´¥å®ä¾‹ - ç®€åŒ–ç‰ˆ')

    parser.add_argument('--failed-file',
                       default='/home/zqq/SWE4CC/failed_instances.txt',
                       help='å¤±è´¥å®ä¾‹IDåˆ—è¡¨æ–‡ä»¶')

    parser.add_argument('--smoke-test', action='store_true',
                       help='åªæµ‹è¯•ç¬¬ä¸€ä¸ªå®ä¾‹')

    parser.add_argument('--output-dir',
                       default='/home/zqq/SWE4CC/results/retry',
                       help='è¾“å‡ºç›®å½•')

    parser.add_argument('--model',
                       default='claude-sonnet-4-5',
                       help='æ¨¡å‹åç§°')

    args = parser.parse_args()

    # åŠ è½½å¤±è´¥å®ä¾‹åˆ—è¡¨
    with open(args.failed_file, 'r') as f:
        failed_instances = [line.strip() for line in f if line.strip()]

    if args.smoke_test:
        print("=" * 70)
        print("ğŸ§ª SMOKE TEST MODE")
        print("=" * 70)
        failed_instances = failed_instances[:1]
        print(f"åªæµ‹è¯•ç¬¬ä¸€ä¸ªå®ä¾‹: {failed_instances[0]}\n")
    else:
        print(f"å°†é‡è¯• {len(failed_instances)} ä¸ªå¤±è´¥å®ä¾‹\n")

    # åˆ›å»ºä¸´æ—¶æ•°æ®é›†
    temp_dataset_path = '/tmp/swebench_retry_dataset'
    count = create_temp_dataset(
        set(failed_instances),
        'princeton-nlp/SWE-bench_Lite',
        temp_dataset_path
    )

    if count == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„å®ä¾‹")
        return 1

    # è¿è¡Œrun_api.py
    print("\n" + "=" * 70)
    print("è¿è¡Œinference...")
    print("=" * 70)

    cmd = [
        'python3', '-m', 'swebench.inference.run_api',
        '--dataset_name_or_path', temp_dataset_path,
        '--model_name_or_path', args.model,
        '--output_dir', args.output_dir
    ]

    print(f"å‘½ä»¤: {' '.join(cmd)}\n")

    try:
        result = subprocess.run(cmd, check=True)

        print("\n" + "=" * 70)
        print("âœ… å®Œæˆ!")
        print("=" * 70)

        # æ£€æŸ¥è¾“å‡º
        output_file = Path(args.output_dir) / f"{args.model}__SWE-bench_Lite__test__default.jsonl"
        if output_file.exists():
            with open(output_file, 'r') as f:
                lines = f.readlines()
            print(f"\nç”Ÿæˆäº† {len(lines)} ä¸ªç»“æœ")
            print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")

            # å¦‚æœæ˜¯smoke testï¼Œæ˜¾ç¤ºç»“æœ
            if args.smoke_test and lines:
                data = json.loads(lines[0])
                has_patch = bool(data.get('model_patch', '').strip())
                print(f"\nSmoke testç»“æœ:")
                print(f"  instance_id: {data['instance_id']}")
                print(f"  æœ‰patch: {'âœ“' if has_patch else 'âœ—'}")
                if has_patch:
                    print(f"  patché•¿åº¦: {len(data['model_patch'])} å­—ç¬¦")

        return 0

    except subprocess.CalledProcessError as e:
        print(f"\nâŒ è¿è¡Œå¤±è´¥: {e}")
        return 1
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­")
        return 1

if __name__ == '__main__':
    sys.exit(main())
