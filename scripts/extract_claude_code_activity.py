#!/usr/bin/env python3
"""
æå–å’Œå±•ç¤ºClaude Codeçš„æ´»åŠ¨è®°å½•
åŒ…æ‹¬æ–‡ä»¶è®¿é—®ã€å·¥å…·ä½¿ç”¨ã€äº¤äº’è½®æ•°ç­‰
"""
import json
import argparse
from pathlib import Path
from typing import Dict, List
import re


def analyze_claude_activity(predictions_file: str):
    """åˆ†æClaude Codeçš„æ´»åŠ¨"""

    print("=" * 80)
    print("Claude Code æ´»åŠ¨åˆ†ææŠ¥å‘Š")
    print("=" * 80)

    with open(predictions_file, 'r') as f:
        for line_num, line in enumerate(f, 1):
            data = json.loads(line)
            instance_id = data['instance_id']

            print(f"\n{'='*80}")
            print(f"ğŸ“Œ Instance: {instance_id}")
            print(f"{'='*80}")

            # 1. åŸºæœ¬ä¿¡æ¯
            claude_meta = data.get('claude_code_meta', {})
            response = claude_meta.get('response_data', {})

            is_enhanced = claude_meta.get('enhanced', False)
            has_tools = claude_meta.get('tools_available', False)

            print(f"\nğŸ“‹ æ¨¡å¼é…ç½®:")
            print(f"  å¢å¼ºæ¨¡å¼: {'âœ… æ˜¯' if is_enhanced else 'âŒ å¦'}")
            print(f"  å·¥å…·å¯ç”¨: {'âœ… æ˜¯' if has_tools else 'âŒ å¦'}")

            # 2. äº¤äº’ç»Ÿè®¡
            num_turns = response.get('num_turns', 0)
            duration_ms = response.get('duration_ms', 0)
            duration_api_ms = response.get('duration_api_ms', 0)

            print(f"\nğŸ”„ äº¤äº’ç»Ÿè®¡:")
            print(f"  äº¤äº’è½®æ•°: {num_turns}")
            print(f"  æ€»è€—æ—¶: {duration_ms/1000:.1f}ç§’")
            print(f"  APIè€—æ—¶: {duration_api_ms/1000:.1f}ç§’")
            print(f"  å¤„ç†è€—æ—¶: {(duration_ms-duration_api_ms)/1000:.1f}ç§’")

            # 3. Tokenä½¿ç”¨
            usage = response.get('usage', {})
            input_tokens = usage.get('input_tokens', 0)
            output_tokens = usage.get('output_tokens', 0)
            cache_creation = usage.get('cache_creation_input_tokens', 0)
            cache_read = usage.get('cache_read_input_tokens', 0)

            print(f"\nğŸ’¬ Tokenä½¿ç”¨:")
            print(f"  Input tokens: {input_tokens:,}")
            print(f"  Output tokens: {output_tokens:,}")
            print(f"  Cache creation: {cache_creation:,} (é¦–æ¬¡è¯»å–)")
            print(f"  Cache read: {cache_read:,} (ç¼“å­˜å‘½ä¸­)")
            print(f"  æ€»è®¡: {input_tokens + output_tokens + cache_creation + cache_read:,}")

            # 4. æ¨¡å‹ä½¿ç”¨ (å¤šæ¨¡å‹è¡¨ç¤ºä½¿ç”¨äº†å·¥å…·)
            model_usage = response.get('modelUsage', {})
            if len(model_usage) > 1:
                print(f"\nğŸ¤– å¤šæ¨¡å‹ä½¿ç”¨ (è¡¨æ˜ä½¿ç”¨äº†å·¥å…·):")
                for model, usage_data in model_usage.items():
                    model_name = model.split('-')[1] if '-' in model else model
                    print(f"  {model_name}:")
                    print(f"    Input: {usage_data.get('inputTokens', 0):,}")
                    print(f"    Output: {usage_data.get('outputTokens', 0):,}")
                    print(f"    Cost: ${usage_data.get('costUSD', 0):.4f}")

            # 5. æƒé™æ‹’ç» (æ˜¾ç¤ºè¢«æ‹’ç»çš„å·¥å…·ä½¿ç”¨)
            denials = response.get('permission_denials', [])
            if denials:
                print(f"\nğŸš« è¢«æ‹’ç»çš„å·¥å…·ä½¿ç”¨:")
                for denial in denials:
                    tool_name = denial.get('tool_name', 'unknown')
                    tool_input = denial.get('tool_input', {})
                    print(f"  âŒ {tool_name}:")
                    if 'command' in tool_input:
                        print(f"     Command: {tool_input['command']}")
                    if 'file_path' in tool_input:
                        print(f"     File: {tool_input['file_path']}")
            else:
                print(f"\nâœ… æ‰€æœ‰å·¥å…·ä½¿ç”¨å‡è¢«å…è®¸")

            # 6. ä»å“åº”å†…å®¹æ¨æ–­æ´»åŠ¨
            result = response.get('result', '')
            full_output = data.get('full_output', '')

            print(f"\nğŸ” æ¨æ–­çš„æ´»åŠ¨ (ä»å“åº”åˆ†æ):")

            # æå–æåˆ°çš„æ–‡ä»¶
            mentioned_files = set()
            file_patterns = [
                r'([a-zA-Z_][a-zA-Z0-9_/]*\.py)',
                r'`([^`]+\.py)`',
                r'"([^"]+\.py)"',
            ]
            for pattern in file_patterns:
                matches = re.findall(pattern, result)
                mentioned_files.update(matches)

            if mentioned_files:
                print(f"\n  ğŸ“ æåˆ°çš„æ–‡ä»¶ ({len(mentioned_files)} ä¸ª):")
                for f in sorted(mentioned_files)[:10]:
                    print(f"    - {f}")
                if len(mentioned_files) > 10:
                    print(f"    ... è¿˜æœ‰ {len(mentioned_files) - 10} ä¸ª")

            # æå–æåˆ°çš„è¡Œå·
            line_mentions = set()
            line_patterns = [
                r'line (\d+)',
                r'at line (\d+)',
                r'on line (\d+)',
            ]
            for pattern in line_patterns:
                matches = re.findall(pattern, result, re.IGNORECASE)
                line_mentions.update(matches)

            if line_mentions:
                print(f"\n  ğŸ”¢ æåˆ°çš„è¡Œå·:")
                lines = sorted([int(l) for l in line_mentions])
                print(f"    {', '.join(map(str, lines[:10]))}")
                if len(lines) > 10:
                    print(f"    ... è¿˜æœ‰ {len(lines) - 10} ä¸ª")

            # å·¥å…·ä½¿ç”¨è¿¹è±¡
            tool_indicators = {
                'Read': ['reading', 'i read', 'after reading', 'the file contains', 'looking at the'],
                'Bash': ['i ran', 'executing', 'running', 'i executed', 'command output'],
                'Grep': ['i searched', 'searching for', 'grep shows', 'found occurrences'],
                'Edit': ['i modified', 'i edited', 'changing', 'updating'],
            }

            detected_tools = []
            for tool, indicators in tool_indicators.items():
                for indicator in indicators:
                    if indicator in result.lower():
                        detected_tools.append(tool)
                        break

            if detected_tools:
                print(f"\n  ğŸ”§ æ£€æµ‹åˆ°å¯èƒ½ä½¿ç”¨çš„å·¥å…·:")
                print(f"    {', '.join(detected_tools)}")

            # 7. Repoä¿¡æ¯
            if 'repo_path' in data:
                print(f"\nğŸ“¦ ä»“åº“ä¿¡æ¯:")
                print(f"  è·¯å¾„: {data['repo_path']}")

            # 8. éªŒè¯ä¿¡æ¯
            if 'validation_errors' in data and data['validation_errors']:
                print(f"\nâš ï¸  éªŒè¯é”™è¯¯ ({len(data['validation_errors'])} ä¸ª):")
                for error in data['validation_errors'][:3]:
                    print(f"  - {error[:80]}...")

            # 9. å°è¯•æ¬¡æ•°
            attempts = data.get('attempts', 1)
            if attempts > 1:
                print(f"\nğŸ”„ é‡è¯•ä¿¡æ¯:")
                print(f"  å°è¯•æ¬¡æ•°: {attempts}")

            # 10. Patchå¤æ‚åº¦
            if 'patch_complexity' in data:
                complexity = data['patch_complexity']
                print(f"\nğŸ“Š Patchå¤æ‚åº¦:")
                print(f"  æ–‡ä»¶æ•°: {complexity.get('files_changed', 0)}")
                print(f"  Hunks: {complexity.get('hunks', 0)}")
                print(f"  æ·»åŠ : {complexity.get('lines_added', 0)} è¡Œ")
                print(f"  åˆ é™¤: {complexity.get('lines_removed', 0)} è¡Œ")

            # 11. æˆæœ¬
            total_cost = response.get('total_cost_usd', data.get('cost', 0))
            print(f"\nğŸ’° æˆæœ¬:")
            print(f"  æœ¬å®ä¾‹: ${total_cost:.4f}")

            # 12. ç”Ÿæˆçš„patché¢„è§ˆ
            patch = data.get('model_patch', '')
            if patch:
                print(f"\nğŸ“ ç”Ÿæˆçš„Patch (å‰5è¡Œ):")
                patch_lines = patch.split('\n')[:5]
                for line in patch_lines:
                    print(f"  {line}")
                if len(patch.split('\n')) > 5:
                    print(f"  ... è¿˜æœ‰ {len(patch.split('\n')) - 5} è¡Œ")

    print(f"\n{'='*80}")
    print("âœ… åˆ†æå®Œæˆ")
    print(f"{'='*80}\n")


def compare_basic_vs_enhanced(basic_file: str, enhanced_file: str):
    """å¯¹æ¯”åŸºç¡€æ¨¡å¼å’Œå¢å¼ºæ¨¡å¼"""

    print("\n" + "=" * 80)
    print("åŸºç¡€æ¨¡å¼ vs å¢å¼ºæ¨¡å¼ å¯¹æ¯”")
    print("=" * 80)

    def load_stats(file):
        with open(file, 'r') as f:
            data = json.loads(f.readline())
            resp = data.get('claude_code_meta', {}).get('response_data', {})
            return {
                'turns': resp.get('num_turns', 0),
                'cache_creation': resp.get('usage', {}).get('cache_creation_input_tokens', 0),
                'cache_read': resp.get('usage', {}).get('cache_read_input_tokens', 0),
                'cost': resp.get('total_cost_usd', 0),
                'models': len(resp.get('modelUsage', {})),
            }

    basic = load_stats(basic_file)
    enhanced = load_stats(enhanced_file)

    print(f"\n| æŒ‡æ ‡ | åŸºç¡€æ¨¡å¼ | å¢å¼ºæ¨¡å¼ | å·®å¼‚ |")
    print(f"|------|---------|---------|------|")
    print(f"| äº¤äº’è½®æ•° | {basic['turns']} | {enhanced['turns']} | +{enhanced['turns']-basic['turns']} |")
    print(f"| Cacheåˆ›å»º | {basic['cache_creation']:,} | {enhanced['cache_creation']:,} | +{enhanced['cache_creation']-basic['cache_creation']:,} |")
    print(f"| Cacheè¯»å– | {basic['cache_read']:,} | {enhanced['cache_read']:,} | +{enhanced['cache_read']-basic['cache_read']:,} |")
    print(f"| ä½¿ç”¨æ¨¡å‹æ•° | {basic['models']} | {enhanced['models']} | +{enhanced['models']-basic['models']} |")
    print(f"| æˆæœ¬ | ${basic['cost']:.4f} | ${enhanced['cost']:.4f} | +${enhanced['cost']-basic['cost']:.4f} |")

    print(f"\nğŸ’¡ è§£è¯»:")
    if enhanced['cache_creation'] > basic['cache_creation']:
        print(f"  â€¢ å¢å¼ºæ¨¡å¼åˆ›å»ºäº† {enhanced['cache_creation']-basic['cache_creation']:,} tokensçš„ç¼“å­˜")
        print(f"    â†’ è¡¨æ˜è¯»å–äº†æ›´å¤šæ–‡ä»¶")
    if enhanced['turns'] > basic['turns']:
        print(f"  â€¢ å¢å¼ºæ¨¡å¼å¤šäº† {enhanced['turns']-basic['turns']} è½®äº¤äº’")
        print(f"    â†’ è¡¨æ˜ä½¿ç”¨äº†å·¥å…·è¿›è¡Œæ¢ç´¢")
    if enhanced['models'] > basic['models']:
        print(f"  â€¢ å¢å¼ºæ¨¡å¼ä½¿ç”¨äº† {enhanced['models']} ä¸ªæ¨¡å‹")
        print(f"    â†’ Haikuç”¨äºå·¥å…·è°ƒç”¨ï¼ŒSonnetç”¨äºä¸»è¦æ¨ç†")


def main():
    parser = argparse.ArgumentParser(description='æå–Claude Codeæ´»åŠ¨è®°å½•')
    parser.add_argument('--predictions_file', required=True, help='Predictionsæ–‡ä»¶')
    parser.add_argument('--compare_with', help='å¯¹æ¯”çš„åŸºç¡€æ¨¡å¼æ–‡ä»¶')

    args = parser.parse_args()

    # åˆ†ææ´»åŠ¨
    analyze_claude_activity(args.predictions_file)

    # å¯¹æ¯”
    if args.compare_with:
        compare_basic_vs_enhanced(args.compare_with, args.predictions_file)


if __name__ == '__main__':
    main()
